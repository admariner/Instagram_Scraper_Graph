{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstagramScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#requests\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "#data, strucuture and maths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import string\n",
    "from  more_itertools import unique_everseen\n",
    "import random\n",
    "\n",
    "#progress,performance and management\n",
    "from tqdm import tqdm_notebook\n",
    "import threading\n",
    "import os\n",
    "import ssl\n",
    "from IPython.display import clear_output\n",
    "import platform\n",
    "import os\n",
    "\n",
    "# imports used in Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#time\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "#text processing / regex\n",
    "import regex\n",
    "import re\n",
    "\n",
    "#make wide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#passwords\n",
    "import getpass\n",
    "\n",
    "\n",
    "\n",
    "class InstagramScraper():\n",
    "\n",
    "    def __init__(self,driver_loc='/Users/sam/Desktop/Chromedriver/chromedriver'):\n",
    "\n",
    "        self.driver_loc = driver_loc\n",
    "\n",
    "        self.userDetails\n",
    "\n",
    "        self.openWebdriver\n",
    "\n",
    "        self.closeWebdriver\n",
    "\n",
    "        self.instagramLogin\n",
    "\n",
    "        self.multithreadCompile\n",
    "\n",
    "        self.multithreadExecute\n",
    "\n",
    "        self.getJson\n",
    "\n",
    "        self.setTarget\n",
    "\n",
    "        self.scrapeLinks\n",
    "\n",
    "        self.postDate\n",
    "\n",
    "        self.postUser\n",
    "\n",
    "        self.postVerifiedUser\n",
    "\n",
    "        self.postLikes\n",
    "\n",
    "        self.postVerifiedTags\n",
    "\n",
    "        self.postUnverifiedTags\n",
    "\n",
    "        self.postComment\n",
    "\n",
    "        self.postLocation\n",
    "\n",
    "        self.postAccessibility\n",
    "\n",
    "        self.logIn\n",
    "\n",
    "        self.getLinks\n",
    "\n",
    "        self.getData\n",
    "\n",
    "    \"\"\"\n",
    "    Multi threading functions\n",
    "    \"\"\"\n",
    "\n",
    "    def multithreadCompile(self,thread_count,iteration_list,func):\n",
    "\n",
    "        jobs = []\n",
    "\n",
    "        #distribute iteration list to batches and append to jobs list\n",
    "        batches = [i.tolist() for i in np.array_split(iteration_list,thread_count)]\n",
    "\n",
    "        for i in range(len(batches)):\n",
    "\n",
    "            jobs.append(threading.Thread(target=func,args=[batches[i]]))\n",
    "\n",
    "\n",
    "        return jobs\n",
    "\n",
    "    def multithreadExecute(self,jobs):\n",
    "\n",
    "        # Start the threads\n",
    "        for j in jobs:\n",
    "            print('execute working')\n",
    "            j.start()\n",
    "\n",
    "        # Ensure all of the threads have finished\n",
    "        for j in jobs:\n",
    "            j.join()\n",
    "        return\n",
    "\n",
    "    \"\"\"\n",
    "    JSON Functions\n",
    "    \"\"\"\n",
    "    #exracts a JSON style dictionary from the html in any given unique Instagram URL\n",
    "    def getJson(self,url):\n",
    "\n",
    "        page = urlopen(url).read()\n",
    "\n",
    "        data=BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        body = data.find('body')\n",
    "\n",
    "        script = body.find('script')\n",
    "\n",
    "        raw = script.text.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "\n",
    "        json_data=json.loads(raw)\n",
    "\n",
    "        return json_data\n",
    "\n",
    "    \"\"\"\n",
    "    Functions that capture log in details and log user into Instagram\n",
    "    \"\"\"\n",
    "    def userDetails(self):\n",
    "\n",
    "        #capture username\n",
    "        username = input('Enter username...')\n",
    "\n",
    "        #capture password\n",
    "        password = getpass.getpass('Enter password...')\n",
    "\n",
    "        self._password = password\n",
    "\n",
    "        self._username = username\n",
    "\n",
    "        return\n",
    "\n",
    "    def openWebdriver(self):\n",
    "\n",
    "        #intiate driver\n",
    "        print(\"Launching driver...\")\n",
    "\n",
    "        driver = webdriver.Chrome(self.driver_loc)\n",
    "\n",
    "        return driver\n",
    "\n",
    "    def closeWebdriver(self,driver):\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def instagramLogin(self,driver):\n",
    "\n",
    "        driver.get('https://www.instagram.com/accounts/login/?source=auth_switcher')\n",
    "\n",
    "        sleep(2)\n",
    "\n",
    "        #log in\n",
    "        username_field = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[2]/div/label/input')\n",
    "\n",
    "        username_field.click()\n",
    "\n",
    "        #send username\n",
    "        username_field.send_keys(self._username)\n",
    "\n",
    "        #locate element to click\n",
    "        try:\n",
    "            password_field = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input')\n",
    "\n",
    "        except:\n",
    "            password_field = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]/div/label/input')\n",
    "\n",
    "        password_field.click()\n",
    "\n",
    "        password_field.send_keys(self._password)\n",
    "\n",
    "        sleep(2)\n",
    "\n",
    "        #find log in button\n",
    "        login_button = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]')\n",
    "\n",
    "        login_button.click()\n",
    "\n",
    "        sleep(3)\n",
    "\n",
    "        #locate floating window to click and close\n",
    "        floating_window = driver.find_element_by_class_name('piCib')\n",
    "\n",
    "        button = floating_window.find_element_by_class_name('mt3GC')\n",
    "\n",
    "        not_now = button.find_element_by_xpath('/html/body/div[4]/div/div/div[3]/button[2]')\n",
    "\n",
    "        not_now.click()\n",
    "\n",
    "        return driver\n",
    "\n",
    "    \"\"\"\n",
    "    Functions that set either a profile or a hashtag as a target and then\n",
    "    scrapes user specified number of post links\n",
    "    \"\"\"\n",
    "    def setTarget(self):\n",
    "\n",
    "        route = input('What do you want to scrape, profile posts or hashtags? (p/h)')\n",
    "\n",
    "        if route == 'h':\n",
    "\n",
    "            hashtag = input('Which hashtag do you want to scrape posts for: ')\n",
    "\n",
    "            self.target_label = '#'+hashtag\n",
    "\n",
    "            tag_url = 'https://www.instagram.com/explore/tags/'\n",
    "\n",
    "            self._target = tag_url+hashtag\n",
    "\n",
    "            return self._target\n",
    "\n",
    "        else:\n",
    "\n",
    "            profile = input('What profile do you want to scrape posts for: ')\n",
    "\n",
    "            self.target_label = '@'+profile\n",
    "\n",
    "            profile_url = 'https://www.instagram.com/'\n",
    "\n",
    "            self._target = profile_url+profile\n",
    "\n",
    "            return self._target\n",
    "\n",
    "    def scrapeLinks(self,url):\n",
    "\n",
    "        #pass url as argument to Selenium webDriver, loads url\n",
    "        self.activedriver.get(url)\n",
    "\n",
    "        options = webdriver.ChromeOptions()\n",
    "\n",
    "        #start maximised\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "\n",
    "        #gets scroll height\n",
    "        last_height = self.activedriver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        #initiate empty list for unique Instagram links\n",
    "        links = []\n",
    "\n",
    "        #some lines for user interactivity / selection of link target(n)\n",
    "        print(\"\\n\")\n",
    "        target = input(\"How many links do you want to scrape (minimum)?: \")\n",
    "        print(\"\\n\")\n",
    "        print(\"Staring Selenium scrape, please keep browser open.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #this loops round until n links achieved or page has ended\n",
    "\n",
    "        while True:\n",
    "\n",
    "            source = self.activedriver.page_source\n",
    "\n",
    "            data= BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "            body = data.find('body')\n",
    "\n",
    "            #script = body.find('span')\n",
    "\n",
    "            for link in body.findAll('a'):\n",
    "\n",
    "                if re.match(\"/p\", link.get('href')):\n",
    "\n",
    "                    links.append('https://www.instagram.com'+link.get('href'))\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Scroll down to bottom\n",
    "            self.activedriver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = self.activedriver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            #if no more content, scrape loop is terminated\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "\n",
    "            #update on successful links scraped\n",
    "            print(\"Scraped \", len(links),\" links, \", len(set(links)),' are unique')\n",
    "\n",
    "            #if n target met then while loop breaks\n",
    "            if len(set(links))>int(target):\n",
    "                break\n",
    "\n",
    "        #links are saved as an attribute for the class instance\n",
    "        self._links = list(unique_everseen(links))\n",
    "\n",
    "        #clear the screen and provide user feedback on performance\n",
    "        clear_output()\n",
    "\n",
    "        print(\"Finished scraping links. Maxed out at \", len(links),\" links, of which \", len(self._links),' are unique.')\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"Unique links obtained. Closing driver\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        # close driver\n",
    "        self.closeWebdriver(self.activedriver)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Methods that extract various fields of data from Instagram JSON dictionaries\n",
    "    \"\"\"\n",
    "    #get date of post\n",
    "    def postDate(self,data):\n",
    "\n",
    "        return datetime.utcfromtimestamp(data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['taken_at_timestamp']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #get user name\n",
    "    def postUser(self,data):\n",
    "\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['owner']['username']\n",
    "\n",
    "    #get verified status\n",
    "    def postVerifiedUser(self,data):\n",
    "\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['owner']['is_verified']\n",
    "\n",
    "    #get how many likes post has got\n",
    "    def postLikes(self,data):\n",
    "\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_preview_like']['count']\n",
    "\n",
    "    #get any verified tags\n",
    "    def postVerifiedTags(self,data):\n",
    "\n",
    "        tag_end_point = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_tagged_user']['edges']\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        verif = []\n",
    "\n",
    "        for i in range(len(tag_end_point)):\n",
    "\n",
    "            entities.append(tag_end_point[i]['node']['user']['full_name'])\n",
    "\n",
    "            verif.append(tag_end_point[i]['node']['user']['is_verified'])\n",
    "\n",
    "        df = pd.DataFrame({'Brand':entities,'Verified':verif})\n",
    "\n",
    "        df = df[df.Verified == True]\n",
    "\n",
    "        if len(list(df.Brand)) < 1:\n",
    "\n",
    "            return np.nan\n",
    "\n",
    "        else:\n",
    "\n",
    "            return list(df.Brand)\n",
    "\n",
    "    #get any unverified tags\n",
    "    def postUnverifiedTags(self,data):\n",
    "\n",
    "        tag_end_point = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_tagged_user']['edges']\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        verif = []\n",
    "\n",
    "        for i in range(len(tag_end_point)):\n",
    "\n",
    "            entities.append(tag_end_point[i]['node']['user']['full_name'])\n",
    "\n",
    "            verif.append(tag_end_point[i]['node']['user']['is_verified'])\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({'Entity':entities,'Verified':verif})\n",
    "\n",
    "        df = df[df.Verified == False]\n",
    "\n",
    "        if len(list(df.Entity)) < 1:\n",
    "\n",
    "            return np.nan\n",
    "\n",
    "        else:\n",
    "\n",
    "            return ''.join(list(df.Entity))\n",
    "\n",
    "    #get post comment\n",
    "    def postComment(self,data):\n",
    "\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "\n",
    "    #get location of post\n",
    "    def postLocation(self,data):\n",
    "\n",
    "        try:\n",
    "\n",
    "            if len(list(data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['location']['name'])) > 0:\n",
    "\n",
    "                return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['location']['name']\n",
    "        except:\n",
    "\n",
    "            return np.nan\n",
    "\n",
    "    #get accessibility  / image data\n",
    "    def postAccessibility(self,data):\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                image = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['accessibility_caption'].replace('Image may contain: ','').replace(' and ',', ').replace('one or more ','')\n",
    "\n",
    "                return image\n",
    "\n",
    "            except:\n",
    "                image = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_sidecar_to_children']['edges'][0]['node']['accessibility_caption'].replace('Image may contain: ','').replace(' and ',', ').replace('one or more ','')\n",
    "\n",
    "                return image\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    #return original post link\n",
    "    def postLink(self,data):\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The three main methods that combine all above\n",
    "    \"\"\"\n",
    "    #get user details, log in and initiate driver\n",
    "    def logIn(self):\n",
    "\n",
    "        self.userDetails()\n",
    "\n",
    "        driver = self.openWebdriver()\n",
    "\n",
    "        self.activedriver = self.instagramLogin(driver)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        print('Successfully logged in..ready to scrape')\n",
    "\n",
    "    #get all the unique links\n",
    "    def getLinks(self):\n",
    "\n",
    "        return self.scrapeLinks(self.setTarget())\n",
    "\n",
    "    #extract data and return dataframe\n",
    "    def getData(self):\n",
    "\n",
    "        #create empty lists for posts and comments\n",
    "        post_date_l = []\n",
    "\n",
    "        post_user_l = []\n",
    "\n",
    "        post_verif_l = []\n",
    "\n",
    "        post_likes_l = []\n",
    "\n",
    "        post_tags_v_l =[]\n",
    "\n",
    "        post_tags_u_l = []\n",
    "\n",
    "        post_l = []\n",
    "\n",
    "        post_location_l = []\n",
    "\n",
    "        post_insta_classifier_l = []\n",
    "\n",
    "        post_link_l = []\n",
    "\n",
    "        self._listStack = [post_date_l,post_user_l,post_verif_l,post_likes_l,post_tags_v_l,\n",
    "                      post_tags_u_l,post_l,post_location_l,post_insta_classifier_l,post_link_l]\n",
    "\n",
    "        self._functionStack = [ self.postDate,\n",
    "                                    self.postUser,\n",
    "                                    self.postVerifiedUser,\n",
    "                                    self.postLikes,\n",
    "                                    self.postVerifiedTags,\n",
    "                                    self.postUnverifiedTags,\n",
    "                                    self.postComment,\n",
    "                                    self.postLocation,\n",
    "                                    self.postAccessibility,\n",
    "                                    self.postLink]\n",
    "\n",
    "        def extractData(links=self._links):\n",
    "\n",
    "            for i in tqdm_notebook(range(len(links))):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    data = self.getJson(links[i])\n",
    "\n",
    "                    for function in self._functionStack:\n",
    "\n",
    "                        if function != self._functionStack[-1]:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                self._listStack[self._functionStack.index(function)].append(function(data))\n",
    "\n",
    "                            except:\n",
    "\n",
    "                                 self._listStack[self._functionStack.index(function)].append(np.nan)\n",
    "                        else:\n",
    "                            self._listStack[-1].append(self._functionStack[-1](links[i]))\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "            return\n",
    "\n",
    "        # execute html parsing fuction using multi threading\n",
    "        print(\"Attemping multi-threading...\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        threads = int(input(\"How many threads?: \"))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"Executing...\")\n",
    "\n",
    "        self.multithreadExecute(self.multithreadCompile(threads,self._links,extractData))\n",
    "\n",
    "        #set up intial data structure\n",
    "        df = pd.DataFrame({'searched_for':[self.target_label]*len(post_l),\n",
    "                           'post_link' :post_link_l,\n",
    "                           'post_date':post_date_l,\n",
    "                           'post':post_l,\n",
    "                           'user':post_user_l,\n",
    "                           'user_verified_status': post_verif_l,\n",
    "                           'post_likes':post_likes_l,\n",
    "                           'post_verified_tags':post_tags_v_l,\n",
    "                           'post_unverified_tags':post_tags_u_l,\n",
    "                           'post_location':post_location_l,\n",
    "                           'post_image':post_insta_classifier_l,\n",
    "\n",
    "                               })\n",
    "\n",
    "#         df['post_hashtags'] = df['post'].map(self.getHashtags)\n",
    "\n",
    "        df.sort_values(by='post_date',ascending=False,inplace=True)\n",
    "\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        self._df = df\n",
    "        return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
